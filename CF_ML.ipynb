{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:18:58.574745Z",
     "start_time": "2019-08-15T16:18:57.848316Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import gc\n",
    "from scipy import sparse\n",
    "from math import sqrt\n",
    "from numpy import random as rd\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set notation of values \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Set directory - where the data is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:18:58.579209Z",
     "start_time": "2019-08-15T16:18:58.576233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_dir = \"D:\\Python\\Thesis\\cf_ready_data\\\\\"\n",
    "os.chdir(_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:18:58.594113Z",
     "start_time": "2019-08-15T16:18:58.580697Z"
    },
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To compare both models we need a metric to unify the algorithms. We use RMSE since this approach provides a nice\n",
    "# meaning in the metric. i.e avg difference between what rating we get vs what we predicted\n",
    "\n",
    "def calc_rmse(actual, pred):\n",
    "    \"\"\"\n",
    "    Inputs: actual, a numpy array of our observed data\n",
    "            pred, a numpy array of our predicted data, for NMF it would be W*H\n",
    "    Output: rmse\n",
    "    \"\"\"\n",
    "    n = actual.sum() # this is we only want to sum entries that exist\n",
    "    se = (actual - pred)**2\n",
    "    \n",
    "    return sqrt((1/n)*se.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:18:58.881317Z",
     "start_time": "2019-08-15T16:18:58.870404Z"
    },
    "code_folding": [
     7
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction function\n",
    "# there are two ways to do prediction: \n",
    "# 1) look at entries of WH on a specific row \n",
    "# 2) np.dot(W[i,:],H[i,:]\n",
    "\n",
    "# point 2 is for comparing other contracts and recommend clauses from them via nearest neighbours\n",
    "\n",
    "def predict(X_hat, actual, ref, filter_actual, idx, n_rec):\n",
    "    \"\"\"\n",
    "    Inputs: X_hat : numpy array, in NMF this is WH, in SVD this is USV\n",
    "            actual: a numpy array, the original X matrix\n",
    "            ref: a pd.DataFrame object, a DataFrame version of X_hat\n",
    "            filter_actual: a boolean value, chooses to get rid of actuals\n",
    "            idx: an integer, contract index\n",
    "            n_rec: an integer, shows top n_rec rows\n",
    "            \n",
    "    Output: pandas DataFrame object, subsetted by idx\n",
    "    \n",
    "    Notes: this function relies on ratings_matrix(), and new_dict3 which is the finalised processed data\n",
    "    new_dict3 = tfidf_summarisation(clauses_list, new_dict2, 10)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create a df for reference to obtain actual clauses\n",
    "    # Idea is to append this to the table so that we preserve clause \n",
    "    #df_to_get_clauses = ratings_matrix(new_dict3, to_df = True, transpose = False, fill_val = 0)\n",
    "    #df_toget_clauses = pd.read_pickle(ref)\n",
    "    df_to_get_clauses = ref\n",
    "    clauses = df_to_get_clauses.columns.values\n",
    "    #  Condition to remove actuals and leave preds\n",
    "    if filter_actual == True:\n",
    "        \n",
    "        diff = X_hat - actual\n",
    "        diff = np.clip(diff,0,1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        diff = X_hat\n",
    "        \n",
    "    print(\"For contract\", idx, \"the top\", n_rec, \"recommended clauses are:\")\n",
    "    \n",
    "    # return the data set subsetted on row by some idx, and sort descending and show n_rec of them\n",
    "    return pd.DataFrame(data=diff[idx,:], index = clauses).sort_values(by=0, axis = 0, ascending = False).head(n_rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading the Ratings Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:19:00.144403Z",
     "start_time": "2019-08-15T16:18:59.800703Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1479, 14427)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file1 = open(\"X3_df.pickle\", \"rb\")\n",
    "X_df = pickle.load(file1)\n",
    "\n",
    "file2 = open(\"X3.pickle\", \"rb\")\n",
    "X = pickle.load(file2)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF\n",
    "\n",
    "Here we employ NMF: Non-negative matrix factorisation.\n",
    "\n",
    "Our goal in NMF is to approximate this matrix by the dot product of two arrays $W$ and $H$. \n",
    "\n",
    "Dimensions of the arrays are defined by dimensions of $X$ and number of components we set to the algorithm. If $X$ has $n$ contracts/rows and $m$ clauses/columns and we want to decompose it to $k$ clauses/columns, then $W$ has $n$ contracts/rows, and $k$ clauses/rows and $H$ has $k$ clauses/rows and $m$ contracts/columns.\n",
    "\n",
    "$X$ is our contract-clauses matrix of dimension $n \\times m$ i.e contracts = rows, clauses = cols\n",
    "\n",
    "$W$ is interpreted as if a contract has clause $y$, what is the additional assignment weight to a group or in our case \"similar-clauses\"\n",
    "\n",
    "$H$ The higher the weight value the more the clause belonging to a group of \"similar-clauses\".\n",
    "\n",
    "Both W,H are initialised as some value - similar to how in NN's weights and biases have an initialisation.\n",
    "\n",
    "Good example and interpretation: https://medium.com/logicai/non-negative-matrix-factorization-for-recommendation-systems-985ca8d5c16c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-15T16:19:02.888039Z",
     "start_time": "2019-08-15T16:19:02.866711Z"
    },
    "code_folding": [
     3,
     5,
     13,
     18,
     20,
     51,
     53,
     58,
     60,
     83,
     94
    ]
   },
   "outputs": [],
   "source": [
    "# Recall that NMF seeks to break down a matrix X into W and H\n",
    "# Such that X ≈ W*H\n",
    "\n",
    "def train_val_NMF_model(data, components, alph, cross_val, method, export, verbose):\n",
    "    \"\"\"\n",
    "    Inputs: data: numpy array object, for fit_transform() method\n",
    "            components: a list object, range of component parameters\n",
    "            alph: a list object, range of regularisation parameters\n",
    "            cross_val: a int object, defines number of k-fold cross val\n",
    "            method: a string object, defines what initialisation is needed for NMF training\n",
    "            export: a boolean value, exports metrics to a dictionary\n",
    "            verbose: a boolean value, turns on verbose on or off\n",
    "            \n",
    "    Outputs: errors: a list of frobenius norm of residual matrix between data and the representation(W,H)\n",
    "             config: a list of configurations used to get the errors\n",
    "             Ws: a list of W components for each configuration of [components, alph] s.t X ≈ W*H\n",
    "             Hs: a list of H components for each configuration of [components, alph] s.t X ≈ W*H\n",
    "    \"\"\"\n",
    "    if type(verbose) != bool:\n",
    "        raise ValueError(\"'verbose' variable is not boolean-type, use 'True' or 'False' to control verbose\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    o_start = time.perf_counter()\n",
    "    errors = []\n",
    "    cv_errors = []\n",
    "    config = []\n",
    "    Ws = []\n",
    "    Hs = []\n",
    "    \n",
    "    print(\"Initialisation:\", method)\n",
    "    print(\"Training and validating...\")\n",
    "    for comp in components:\n",
    "        start = time.perf_counter()\n",
    "        for alphas in alph:\n",
    "            print(\"Config (components, alpha):\",[comp,alphas])\n",
    "            for cv in range(cross_val):\n",
    "                np.random.shuffle(data) # shuffle data to perform cv and get rmse\n",
    "                NMF_model = NMF(\n",
    "                            verbose = verbose,\n",
    "                            n_components = comp,\n",
    "                            init = method, \n",
    "                            solver = 'mu',\n",
    "                            beta_loss = 'frobenius', # also called Euclidean Norm\n",
    "                            tol = 1e-4,\n",
    "                            random_state = 0,\n",
    "                            alpha = alphas,\n",
    "                            max_iter = 100\n",
    "                           )\n",
    "            \n",
    "                if verbose == True:\n",
    "                    print(\"Computing W...\")\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "                W = NMF_model.fit_transform(data)\n",
    "        \n",
    "                if verbose == True:\n",
    "                    print(\"Computing H...\")\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "                H = NMF_model.components_\n",
    "\n",
    "                cv_error = calc_rmse(X, (W@H))\n",
    "                #error = sqrt(error.mean())\n",
    "                print(cv,\"-fold\" ,\"train_rmse:\", cv_error)\n",
    "                cv_errors.append(cv_error)\n",
    "                \n",
    "            print(\"Avg rmse:\", np.mean(cv_error)) \n",
    "            print(\"Training time elapsed in minutes: \", (time.perf_counter() - start)/60) \n",
    "            print(\"\")\n",
    "            errors.append(np.mean(cv_error))\n",
    "            config.append([comp, alphas])\n",
    "            Ws.append(sparse.csr_matrix(W))\n",
    "            Hs.append(sparse.csr_matrix(H))\n",
    "            gc.collect()\n",
    "    \n",
    "    argmin_error = np.argmin(errors)\n",
    "    best_config = config[argmin_error]\n",
    "    best_error = errors[argmin_error]\n",
    "    \n",
    "    if export == True:\n",
    "        metrics = {\"best_train_config\": best_config,\n",
    "                   \"best_train_rmse\": best_error,\n",
    "                   \"train_config\": config,\n",
    "                   \"train_rmse\": errors,\n",
    "                   \"Ws\": Ws,\n",
    "                   \"Hs\": Hs      \n",
    "                  }\n",
    "        out = open(\"D:\\Python\\Thesis\\metrics\\\\\" + \"nmf_\" + 'metrics.pickle' ,\"wb\")\n",
    "        pickle.dump(metrics, out)\n",
    "        out.close\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"Training and validating complete\")        \n",
    "    print(\"Total time elapsed in minutes: \", (time.perf_counter() - o_start)/60) \n",
    "    print(\"\")\n",
    "    print(\"Best configuration:\", best_config, \"with error:\", best_error)\n",
    "    print(\"Subset W, H at index:\", argmin_error)\n",
    "    print(\"---------------------------------------\")\n",
    "    return errors, config, Ws, Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-15T16:19:03.271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation: nndsvdar\n",
      "Training and validating...\n",
      "Config (components, alpha): [100, 0.01]\n",
      "0 -fold train_rmse: 0.8162168782884001\n",
      "1 -fold train_rmse: 0.8161847382636858\n",
      "2 -fold train_rmse: 0.8160648100509018\n",
      "3 -fold train_rmse: 0.8159249892409699\n",
      "4 -fold train_rmse: 0.817024643043419\n",
      "Avg rmse: 0.817024643043419\n",
      "Training time elapsed in minutes:  0.6121474500499999\n",
      "\n",
      "Config (components, alpha): [200, 0.01]\n",
      "0 -fold train_rmse: 0.7141081205834762\n",
      "1 -fold train_rmse: 0.7140116166454343\n",
      "2 -fold train_rmse: 0.7139630770884106\n",
      "3 -fold train_rmse: 0.7145120248939532\n",
      "4 -fold train_rmse: 0.7138248536071132\n",
      "Avg rmse: 0.7138248536071132\n",
      "Training time elapsed in minutes:  1.2418213757499998\n",
      "\n",
      "Config (components, alpha): [300, 0.01]\n",
      "0 -fold train_rmse: 0.6373105998997659\n",
      "1 -fold train_rmse: 0.6380719644330592\n",
      "2 -fold train_rmse: 0.6368708531298146\n",
      "3 -fold train_rmse: 0.6373709813106713\n",
      "4 -fold train_rmse: 0.6376457437282267\n",
      "Avg rmse: 0.6376457437282267\n",
      "Training time elapsed in minutes:  2.0660168736833335\n",
      "\n",
      "Config (components, alpha): [400, 0.01]\n",
      "0 -fold train_rmse: 0.5718230769982443\n",
      "1 -fold train_rmse: 0.5722786698394994\n",
      "2 -fold train_rmse: 0.5709616443662954\n",
      "3 -fold train_rmse: 0.5718664555443598\n",
      "4 -fold train_rmse: 0.5719766942227067\n",
      "Avg rmse: 0.5719766942227067\n",
      "Training time elapsed in minutes:  2.840094080316667\n",
      "\n",
      "Config (components, alpha): [500, 0.01]\n",
      "0 -fold train_rmse: 0.5125421234244377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\pycharm 2019.2\\venv\\lib\\site-packages\\sklearn\\decomposition\\nmf.py:1069: ConvergenceWarning: Maximum number of iteration 100 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -fold train_rmse: 0.5100578317413397\n",
      "2 -fold train_rmse: 0.5102032222798351\n",
      "3 -fold train_rmse: 0.5111885077766113\n",
      "4 -fold train_rmse: 0.5104031106876948\n",
      "Avg rmse: 0.5104031106876948\n",
      "Training time elapsed in minutes:  4.351201239883332\n",
      "\n",
      "Config (components, alpha): [600, 0.01]\n",
      "0 -fold train_rmse: 0.45299057479683197\n",
      "1 -fold train_rmse: 0.4527254713764287\n"
     ]
    }
   ],
   "source": [
    "comp = [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400]\n",
    "#comp = [300]\n",
    "r_error, r_configs, r_Ws, r_Hs = train_val_NMF_model(data = X,\n",
    "                                                     components = comp, \n",
    "                                                     alph = [0.01], \n",
    "                                                     cross_val = 5,\n",
    "                                                     method = 'nndsvdar',\n",
    "                                                     export = True,\n",
    "                                                     verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NMF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T15:36:44.478318Z",
     "start_time": "2019-08-14T15:36:44.012078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NMF_Xhat = r_Ws[14].todense()@r_Hs[14].todense()\n",
    "#pd.DataFrame(NMF_Xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "Core idea of SVD is similar to NMF where we want to express our contract-clauses matrix as a product of matrices in a smaller dimension.\n",
    "\n",
    "The only difference is the training process and the components we obtain. In NMF we obtain two components $W,H$ and in SVD we obtain three components $U,S,V^T$. SVD components are obtained via linear algebra techniques.\n",
    "\n",
    "But there is very little interpretability - hard to explain to non-technical people what is going on.\n",
    "\n",
    "In addition to looking at the entries for predictions, SVD allows approach allows us to project a specific contract into a smaller space and thus compare contracts (via some distance metric) and get recommendations from similar contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-15T16:19:09.372Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "# Here we use SVD approach, idea is we want to decompose X = UΣV^(T)\n",
    "\n",
    "def train_val_SVD_model(data, components, cross_val, export):\n",
    "    \"\"\"\n",
    "    Inputs: data: numpy array object, for fit_transform() method\n",
    "            components: a list object, range of component parameters\n",
    "            cross_val: a int object, defines number of k-fold cross val\n",
    "            export: a boolean value, exports metrics\n",
    "            \n",
    "    Outputs: errors: a list of frobenius norm of residual matrix between data and the representation(W,H)\n",
    "             config: a list of configurations used to get the errors\n",
    "\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    config = []\n",
    "    U_list = []\n",
    "    S_list = []\n",
    "    V_t_list = []\n",
    "    cv_rmse_list = []\n",
    "    avg_rmse_list = []\n",
    "    \n",
    "    print(\"Performing tSVD...\")\n",
    "    for comp in components:\n",
    "        print(\"Number of components:\", comp)\n",
    "        for cv in range(cross_val):\n",
    "            np.random.shuffle(data) # shuffle data to perform cv and get rmse\n",
    "            U, S_placeholder, V_t = randomized_svd(\n",
    "                        M = data,\n",
    "                        n_components = comp,\n",
    "                        random_state = 0\n",
    "                        )\n",
    "            # sklearn returns a list of components, but it should be in a matrix where these values are in diagonal entries\n",
    "            S = np.ndarray(shape = (S_placeholder.shape[0],S_placeholder.shape[0]))\n",
    "            np.fill_diagonal(S, S_placeholder)\n",
    "\n",
    "            # Reconstruction of the data\n",
    "            data_pred = U@S@V_t\n",
    "\n",
    "            rmse = calc_rmse(data, data_pred)\n",
    "        \n",
    "            print(cv,\"fold\",\"RMSE:\", rmse)\n",
    "            print(\"\")\n",
    "        \n",
    "            cv_rmse_list.append(rmse)\n",
    "        \n",
    "        print(\"Avg rmse:\", np.mean(cv_rmse_list))\n",
    "        avg_rmse_list.append(np.mean(cv_rmse_list))\n",
    "        config.append([comp])\n",
    "        U_list.append(sparse.csr_matrix(U))\n",
    "        S_list.append(sparse.csr_matrix(S))\n",
    "        V_t_list.append(sparse.csr_matrix(V_t))\n",
    "    \n",
    "    lowest_rmse_idx = np.argmin(avg_rmse_list)\n",
    "    \n",
    "    if export == True:\n",
    "        metrics = {\"best_idx\": lowest_rmse_idx,\n",
    "                   \"best_config\": config[lowest_rmse_idx],\n",
    "                   \"best_U\": U_list[lowest_rmse_idx],\n",
    "                   \"best_S\": S_list[lowest_rmse_idx],\n",
    "                   \"best_Vt\":V_t_list[lowest_rmse_idx]\n",
    "                  }\n",
    "        out = open(\"D:\\Python\\Thesis\\metrics\\\\\" + \"svd_\" + '_metrics.pickle' ,\"wb\")\n",
    "        pickle.dump(metrics, out)\n",
    "        out.close\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"tSVD complete\") \n",
    "    print(\"Best # of components to choose is\", config[lowest_rmse_idx],\",\",\"Subset on index:\",lowest_rmse_idx)\n",
    "    print(\"Time elapsed in minutes: \", (time.perf_counter() - start)/60) \n",
    "    \n",
    "    return config, U_list, S_list, V_t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-15T16:19:09.556Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "conf, U, S, V = train_val_SVD_model(X, comp, export = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T13:04:26.252626Z",
     "start_time": "2019-08-14T13:02:57.198Z"
    }
   },
   "outputs": [],
   "source": [
    "#SVD_Xhat = U[]@S[]@V[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T13:04:26.253620Z",
     "start_time": "2019-08-14T13:02:57.200Z"
    }
   },
   "outputs": [],
   "source": [
    "#predict(SVD_Xhat, X, True, 21, 10) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
