{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import modules & Directory config\n",
    "These are Python modules which are needed (most of them) for the cleansing to happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:39:22.771303Z",
     "start_time": "2019-08-12T11:39:07.623202Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dangj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dangj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dangj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of threads for multiprocessing: 8\n"
     ]
    }
   ],
   "source": [
    "# Import standard modules\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "from multiprocessing import cpu_count\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "# Import NLP modules for pre-processing\n",
    "import spacy\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Enable these if you have not downloaded nltk packages before\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set notation of values \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Multithread processing\n",
    "num_threads = threading.activeCount()\n",
    "print(\"# of threads for multiprocessing:\", cpu_count())\n",
    "\n",
    "# Assign multithread processing\n",
    "pool = ThreadPool(cpu_count()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Set a working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:39:22.777386Z",
     "start_time": "2019-08-12T11:39:22.773288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set working directory, i.e where the data is located\n",
    "directory = \"D:\\Python\\Thesis\\data\"\n",
    "\n",
    "os.chdir(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data parsing functions\n",
    "Functions mainly to create an identifier and normalise some of the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:06:06.511755Z",
     "start_time": "2019-08-11T14:06:06.287092Z"
    },
    "code_folding": [
     0,
     12,
     24,
     31
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def left(s, amount):\n",
    "    \"\"\"\n",
    "    Inputs: s - a string, amount - an integer\n",
    "    Output: returns characters of a string, starting from the left\n",
    "    Example: \n",
    "    s = 'string'\n",
    "    amount = 3\n",
    "    print(left(s,amount))\n",
    "    # 'str'\n",
    "    \"\"\"\n",
    "    return s[:amount]\n",
    "\n",
    "def right(s, amount):\n",
    "    \"\"\"\n",
    "    Inputs: s - a string, amount - an integer\n",
    "    Output: returns characters of a string, starting from the right\n",
    "    Example: \n",
    "    s = 'string'\n",
    "    amount = 3\n",
    "    print(right(s,amount))\n",
    "    # 'ing'\n",
    "    \"\"\"\n",
    "    return s[-amount:]\n",
    "\n",
    "def process_token(token):\n",
    "    \"\"\"lower cases tokens\"\"\"\n",
    "    return token.lemma_.lower()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\"])\n",
    "regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "def alignment_pipeline(clause):\n",
    "    tokens = nlp(clause)\n",
    "    \n",
    "    return ' '.join(regex.sub('', process_token(token)) for token in tokens if not token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating the data in dict format and store locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:06:13.779134Z",
     "start_time": "2019-08-11T14:06:13.765222Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#def create_data(pickl, location):\n",
    "    \"\"\"\n",
    "    Input: pickl: a boolean value, True = saves dictionary to a .pickle format\n",
    "           location: a string value, directory of the data, accept wild cards and must include file itself\n",
    "    Output: a dictionary that has preprocessed clauses.\n",
    "    notes: example location: 'selected_contracts/warrant/*.clauses'\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    raw_clauses = defaultdict(list)\n",
    "    count = 0\n",
    "    files = glob(location, recursive=True)\n",
    "    \n",
    "    for file_path in tqdm(files, position = 0, desc = \"Compiling contracts\"):\n",
    "        size = os.stat(f\"{file_path}\").st_size\n",
    "        \n",
    "        if size > 0:\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    # Obtain/create the contract ID rather than the full directory\n",
    "                    contract = left(file_path, (len(file_path) - len(right(file_path, len('.fclauses')))))\n",
    "                    contract = contract.split(\"\\\\\" )[2]\n",
    "                    \n",
    "                    # Next iteration\n",
    "                    count += 1\n",
    "            \n",
    "                    # append ID and the clauses\n",
    "                    raw_clauses[contract].append(line.strip())\n",
    "    \n",
    "    \n",
    "    # list to store clauses\n",
    "    clauses = []\n",
    "    \n",
    "    for key in tqdm(raw_clauses.keys(), position = 0, desc = \"Processing contracts\"):\n",
    "        for i in range(len(raw_clauses[f\"{key}\"])):\n",
    "            \n",
    "            # repeated alignment for extra white spaces\n",
    "            raw_clauses[f\"{key}\"][i] = alignment_pipeline(raw_clauses[f\"{key}\"][i])\n",
    "            #raw_clauses[f\"{key}\"][i] = alignment_pipeline(raw_clauses[f\"{key}\"][i])\n",
    "            clauses.append(raw_clauses[f\"{key}\"][i])\n",
    "            \n",
    "    \n",
    "    if pickl == True:\n",
    "        pickle_out = open(\"full_raw_data_parse2.pickle\", \"wb\")\n",
    "        pickle.dump(dict(raw_clauses), pickle_out)\n",
    "        print(\"Dictionary has been pickled to:\", os.getcwd() +\"/full_raw_data_parse2.pickle\")\n",
    "    else:\n",
    "        print(\"Dictionary has not been pickled\")\n",
    "        pass\n",
    "  \n",
    "    print(\"Time elapsed in seconds: \", time.perf_counter() - start)\n",
    "    \n",
    "    return dict(raw_clauses), clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:27:35.137643Z",
     "start_time": "2019-08-07T12:27:31.387915Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#raw_data, raw_clauses = create_data(pickl = True, location = 'D:/Python/Thesis/selected_contracts/*/*.fclauses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Cleansing\n",
    "\n",
    "- Remove duplicate contracts\n",
    "- Remove \"empty\" clauses\n",
    "- Remove very short clauses\n",
    "- Remove contracts that are considered as outliers/or been incorrectly parsed (all clauses in one list or too many clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Removing duplicate contracts\n",
    "Approach here is to concatenate clauses together and compute a measure of similarity (cosine similarity, 0 = different, 1 = identical) apply a threshold and then remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:27:35.169414Z",
     "start_time": "2019-08-07T14:27:35.139131Z"
    },
    "code_folding": [
     0,
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def simplify_contracts(dictionary):\n",
    "    \"\"\"\n",
    "    Input: dictionary, A dictionary of the contract data\n",
    "    Output: processed_data , A dictionary which has a key, and a concatenated string of clauses from dictionary. \n",
    "    \n",
    "    Notes: Simplifies contracts, a contract is defined to be a dictionary with an ID for the contract as the key\n",
    "           and multiple values in the form of lists containing strings. simplify_contracts() concatenates the lists\n",
    "           into a single list.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    processed_data = defaultdict(list)\n",
    "    contract_list= []\n",
    "    \n",
    "    for key in tqdm(dictionary.keys(), position = 0, desc = 'Preparing for duplicate detection'):\n",
    "\n",
    "        concat_clauses = ''\n",
    "        for clause in range(len(dictionary[f\"{key}\"])):\n",
    "            \n",
    "            concat_clauses = concat_clauses + dictionary[f\"{key}\"][clause]\n",
    "                       \n",
    "        processed_data[f\"{key}\"].append(concat_clauses)\n",
    "        #processed_data[clause] = processed_data.pop(f\"{key}\")\n",
    "        # for some reason the order gets changed temporarily\n",
    "        contract_list.append(concat_clauses)\n",
    "            \n",
    "    print(\"Time elapsed in seconds: \", time.perf_counter() - start)    \n",
    "    \n",
    "    return dict(processed_data), contract_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:27:36.114820Z",
     "start_time": "2019-08-07T14:27:35.170874Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "process_data, process_contract_list = simplify_contracts(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:08.048349Z",
     "start_time": "2019-08-07T14:27:36.116310Z"
    },
    "code_folding": [
     0,
     2,
     18,
     20,
     42,
     44,
     52,
     54,
     62,
     64,
     67,
     73
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dedupe(dictionary, process_data_dict, process_contract_list, thresh, verbose):\n",
    "    \"\"\"\n",
    "    Inputs: dictionary, a dictionary object of contracts\n",
    "            process_data_dict, a dictionary object of processed contracts from 1st object returned by simplify_contracts()\n",
    "            process_contract_list, a list object of processed contracts from 2nd object returned by simplify_contracts()\n",
    "            thresh, a positive real number between [0,1]; determines what contracts should be determined as a duplicate\n",
    "            verbose, a boolean value to turn on/off progress\n",
    "            \n",
    "    Output: cleansed_data, a dictionary that has the duplicate contracts removed   \n",
    "    \"\"\"\n",
    "    # Used for giving the run time of function\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    # Create class of tfidf packages\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Apply count vectorizer to the contract list\n",
    "    if verbose == True:\n",
    "        print(\"Fitting and Transforming TF-IDF onto dataset...\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    count_vec = count_vectorizer.fit_transform(process_contract_list)\n",
    " \n",
    "    # Create list of contract ids\n",
    "    contract_ids = list(process_data.keys())\n",
    "   \n",
    "    # Cosine similarity between contracts\n",
    "    \n",
    "    sim_matrix = [] # placeholder to store similarities\n",
    "    \n",
    "    # Need to loop over each document row and compute similarity due to memory restrictions\n",
    "    for i in tqdm(range(count_vec.shape[0]), position = 0, desc = 'Computing Cosine Similarity' ):\n",
    "        sim = cosine_similarity(count_vec[0,:].toarray(), count_vec[i,:].toarray())[0][0]\n",
    "        sim_matrix.append(sim)   \n",
    "    \n",
    "    sim_matrix = np.array(sim_matrix[1:]) # subset from index 1 and above because we'll get rid of the original if we index all rows\n",
    "    \n",
    "    # Thresholding the similarity\n",
    "    # Change thresholding the data, initial value is 0.9\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Thresholding...\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    sim_matrix[sim_matrix >= thresh] = 1\n",
    "\n",
    "    # Check to see how many dupes there are\n",
    "    count_dupe = np.sum(sim_matrix[sim_matrix == 1])\n",
    "  \n",
    "    if verbose == True:\n",
    "        print(\"# of duplicate contracts: \", count_dupe)\n",
    "    else:\n",
    "        pass\n",
    "     \n",
    "    # Create list of duplicate contract ids\n",
    "    dupe = np.where(sim_matrix >= 1)[0]\n",
    "    dupe = dupe.tolist()\n",
    "    dupe_contracts = [contract_ids[i] for i in dupe]\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Deduping in-place...\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    for key in dupe_contracts:\n",
    "        \n",
    "        dictionary.pop(f\"{key}\")\n",
    "       \n",
    "    if verbose == True:\n",
    "        print(\"dictionary has been de-duped in place, no need to store in variable\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"Time elapsed in mins: \", (time.perf_counter() - start)/60) \n",
    "    \n",
    "    return \n",
    " \n",
    "# Start de-duping the data in-place\n",
    "dedupe(raw_data, process_data, process_contract_list, 0.8, True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Removing empty clauses\n",
    "Approach here is that since this dictionary has keys, and values are lists, the list length must be 0 so we remove these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:08.556722Z",
     "start_time": "2019-08-07T14:34:08.049808Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_dict = defaultdict(list)\n",
    "for key in tqdm(raw_data.keys(), position = 0, desc = 'Removing Empty Clauses'):\n",
    "    for i in range(len(raw_data[f\"{key}\"])):\n",
    "        if len(raw_data[f\"{key}\"][i]) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            new_dict[f\"{key}\"].append(raw_data[f\"{key}\"][i])  \n",
    "\n",
    "# THE DATA IS NOW HELD IN A VARIABLE CALLED new_dict\n",
    "#pd.DataFrame.from_dict(new_dict, 'index').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Removing very short/long clauses\n",
    "Here we remove entries where there clauses don't meet an adequate length, in this case, clauses are removed if the are $\\not\\in [10,100]$ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:33.320033Z",
     "start_time": "2019-08-07T14:34:08.558210Z"
    },
    "code_folding": [
     9
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_dict2 = defaultdict(list)\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Define a threshold which contributes to a clause\n",
    "word_count_min = 10\n",
    "word_count_max = 100\n",
    "# Define the tokenizer - using nltk since spacy is slower \n",
    "tokenizer = nltk.RegexpTokenizer('\\s+', gaps=True)\n",
    "\n",
    "for key in tqdm(new_dict.keys(), position = 0, desc = \"Removing very short/long clauses\"):\n",
    "    for i in range(len(new_dict[f\"{key}\"])):\n",
    "        if word_count_min < len(tokenizer.tokenize(new_dict[f\"{key}\"][i])) <= word_count_max:\n",
    "            # Put clauses of word length between [10,100] in\n",
    "            new_dict2[f\"{key}\"].append(new_dict[f\"{key}\"][i])\n",
    "\n",
    "        else:\n",
    "            # Do not put long ones in\n",
    "            pass\n",
    "print(\"Time elapsed in mins: \", (time.perf_counter() - start)/60)\n",
    "#pd.DataFrame.from_dict(new_dict2, orient='index').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Resolving contracts that are considered as outliers\n",
    "We look at how many 'clauses' each contract has and then decide how resolve the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:38.090092Z",
     "start_time": "2019-08-07T14:34:33.321521Z"
    },
    "code_folding": [
     2,
     13
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = np.zeros(len(new_dict2)) , index = new_dict2.keys(), columns = [\"Clauses\"])\n",
    "\n",
    "for key in new_dict2.keys():\n",
    "    df.loc[f\"{key}\"] = len(new_dict2[f\"{key}\"])\n",
    "    \n",
    "df.head()\n",
    "\n",
    "# Create list of outliers where we have < 3 clauses\n",
    "clause_thresh = 3\n",
    "outlier_list = list(df[df.Clauses <= clause_thresh].index)\n",
    "\n",
    "df = df.drop(df[df.Clauses <= 3].index)\n",
    "\n",
    "while True:\n",
    "    outliers = list(df[df['Clauses'] > df['Clauses'].mean() + 3 * df['Clauses'].std()].index)\n",
    "    #print(outliers)\n",
    "    \n",
    "    if len(outliers) > 0:\n",
    "        # Keep a copy of a list of outliers to remove from dictionary\n",
    "        outlier_list.append(outliers[0])\n",
    "        # Remove from the data frame\n",
    "        df = df.drop(labels = outliers)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "# Boxplot of the # of clauses        \n",
    "sns.boxplot(x = df['Clauses'])\n",
    "print(df.describe())\n",
    "\n",
    "# List of outliers\n",
    "print(\"# of Outliers detected:\", len(outlier_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:38.101996Z",
     "start_time": "2019-08-07T14:34:38.091061Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Here we truncate the outliers to the median value\n",
    "\n",
    "median = int(np.round(df.median()[0],0))\n",
    "\n",
    "for outlier in outlier_list:\n",
    "    new_dict2[f\"{outlier}\"] = new_dict2[f\"{outlier}\"][0:median]\n",
    "    \n",
    "#pd.DataFrame.from_dict(new_dict2, orient='index').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Export cleansed data - NO SUMMARISATION APPLIED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:34:39.822346Z",
     "start_time": "2019-08-07T14:34:38.103506Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Save the cleansed dataset\n",
    "#pickle_out = open(\"full_cleansed_raw_data_parse2.pickle\", \"wb\")\n",
    "#pickle.dump(new_dict2, pickle_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Text summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:09:48.690036Z",
     "start_time": "2019-08-11T14:09:48.326939Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle_in = open(\"full_cleansed_raw_data_parse2.pickle\", \"rb\")\n",
    "new_dict2 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T14:09:52.228914Z",
     "start_time": "2019-08-11T14:09:52.038481Z"
    },
    "code_folding": [
     2
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clauses_list = []\n",
    "cls = 0\n",
    "for key in new_dict2.keys():\n",
    "    for clause in range(len(new_dict2[f\"{key}\"])):\n",
    "        clauses_list.append(new_dict2[f\"{key}\"][clause])\n",
    "        cls+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Use TFIDF to summarise clauses and reduce columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-11T20:01:49.747165Z",
     "start_time": "2019-08-11T14:10:17.225025Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and Transforming TF-IDF onto dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarising clauses: 100%|██████████████████████████████████████████████████████████████| 565235/565235 [5:51:17<00:00, 26.82it/s]\n",
      "Creating summarised dict: 100%|█████████████████████████████████████████████████████████| 50106/50106 [00:00<00:00, 235994.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary has been pickled to: D:\\Python\\Thesis\\data/full_summarised_data_parse2.pickle\n"
     ]
    }
   ],
   "source": [
    "def tfidf_summarisation(clause_list, new_dict, n, pickl):\n",
    "    \"\"\"\n",
    "    Inputs: clause_list: a list object, which contains clauses at every index\n",
    "            new_dict: a dictionary object, A dictionary of the contract data\n",
    "            n: a natural number, used to apply thresholding for top n words\n",
    "            pickle: a boolean, used to pickle dictionary object\n",
    "            \n",
    "    Output: new_dict3: a dictionary object where the clauses are summarised via tf-idf\n",
    "    \"\"\"\n",
    "    summarised_clauses = []\n",
    "    \n",
    "    # Create class of tfidf packages\n",
    "    print(\"Fitting and Transforming TF-IDF onto dataset...\")\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "    # Apply count vectorizer to the contract list\n",
    "    count_vec = count_vectorizer.fit_transform(clause_list)\n",
    "\n",
    "    # Create dataframe containing contracts x words - due to memory errors need to loop can't instantly create matrix\n",
    "    \n",
    "    for i in tqdm(range(len(clause_list)), position = 0, desc = \"Summarising clauses\"):\n",
    "        # look at every row and obtain the full format i.e dense form, \n",
    "        # sort in descending order, lock index at 0 to bypass 'by' in sort_values()\n",
    "        row_slice = count_vec[i,:].todense()\n",
    "        row_slice = pd.DataFrame(row_slice, columns = count_vectorizer.get_feature_names())\n",
    "        row_slice = row_slice.iloc[0,:].sort_values(ascending = False)\n",
    "        \n",
    "        # obtain top n tf-idf words as a list\n",
    "        row_topn = ' '.join(row_slice[0:n].index.to_list())\n",
    "        \n",
    "        # append to summarised_clauses\n",
    "        summarised_clauses.append(row_topn)\n",
    "    \n",
    "    \n",
    "    # Build a new dict for the summarised clauses\n",
    "    new_dict3 = defaultdict(list)\n",
    "    \n",
    "    idx = 0\n",
    "    for key in tqdm(new_dict.keys(), position = 0, desc = \"Creating summarised dict\"):\n",
    "        for clause in range(len(new_dict[f\"{key}\"])):\n",
    "            \n",
    "            new_dict3[key].append(summarised_clauses[idx])\n",
    "            idx +=1\n",
    "    \n",
    "    if pickl == True:\n",
    "        pickle_out = open(\"full_summarised_data_parse2.pickle\",\"wb\")\n",
    "        pickle.dump(new_dict3, pickle_out)\n",
    "        print(\"Dictionary has been pickled to:\", os.getcwd() +'/full_summarised_data_parse2.pickle')\n",
    "    else:\n",
    "        print(\"Dictionary has not been pickled\")\n",
    "        pass\n",
    "\n",
    "    return new_dict3\n",
    "\n",
    "new_dict3 = tfidf_summarisation(clauses_list, new_dict2, 3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load up the processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:25:47.736837Z",
     "start_time": "2019-08-12T11:25:47.125269Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load full cleansed data\n",
    "f_data = open(\"full_cleansed_raw_data_parse2.pickle\",'rb')\n",
    "full_data = pickle.load(f_data)\n",
    "\n",
    "# Load summarised data\n",
    "summarised_data = open(\"full_summarised_data_parse2.pickle\",'rb')\n",
    "summ_data = pickle.load(summarised_data)\n",
    "\n",
    "# Memory saver\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Create and save sampled version of full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:26:45.387006Z",
     "start_time": "2019-08-12T11:26:45.381524Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Sampler(Data, p, seed, replacement, save, loc, data_type):\n",
    "    \"\"\"\n",
    "    Inputs: Data = a dict object, containing a key (contract id) and it's values (clauses)\n",
    "            p = a float object, a number between 0-1 to represent the % of data sampled\n",
    "            seed = an int object, a number to represent the random state for reproducible results\n",
    "            replacement = a boolean object, to sample with or without replacement\n",
    "            save = a boolean object, determines to save the file or not\n",
    "            loc = a string object, location of where to save file\n",
    "            data_type = string object, whether the file to save is the full or summarised version\n",
    "            \n",
    "    Outputs: a dictionary version of the sampled data\n",
    "    \"\"\"\n",
    "    print(\"Reading DataFrame from Dictionary...\")\n",
    "    df = pd.DataFrame.from_dict(data = Data, orient = 'index')\n",
    "    print(\"Sampling DataFrame with parameters:\", \"% sampled:\", p*100, \"random state:\", seed)\n",
    "    df = df.sample(frac = p, replace = replacement, random_state = seed).T\n",
    "    \n",
    "    if save == True and type(loc) == str:\n",
    "        print(\"Saving dictionary...\")\n",
    "        pickle_sample_out = open(f\"{loc}\" + f\"{data_type}_\" +\"sample_\" + f\"{p*100}\" + \".pickle\",\"wb\")\n",
    "        pickle.dump(df.to_dict(orient = 'index'), pickle_sample_out)\n",
    "        print(\"Save completed!\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:29:54.806109Z",
     "start_time": "2019-08-12T11:29:24.248396Z"
    },
    "code_folding": [
     1
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading DataFrame from Dictionary...\n",
      "Sampling DataFrame with parameters: % sampled: 1.0 random state: 1234\n",
      "Saving dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program files\\pycharm 2019.2\\venv\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save completed!\n",
      "Reading DataFrame from Dictionary...\n",
      "Sampling DataFrame with parameters: % sampled: 1.0 random state: 1234\n",
      "Saving dictionary...\n",
      "Save completed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sampling summarised data\n",
    "Sampler(Data = summ_data, \n",
    "        p = 0.01, \n",
    "        seed = 1234, \n",
    "        replacement = True,\n",
    "        save = True,\n",
    "        loc = \"D:\\Python\\Thesis\\data_samples\\\\\",\n",
    "        data_type = \"summ\"\n",
    "       )\n",
    "# Sampling full data\n",
    "\n",
    "Sampler(Data = full_data, \n",
    "        p = 0.01, \n",
    "        seed = 1234, \n",
    "        replacement = True,\n",
    "        save = True,\n",
    "        loc = \"D:\\Python\\Thesis\\data_samples\\\\\",\n",
    "        data_type = \"full\"\n",
    "       )\n",
    "\n",
    "# Memory saver\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Export ratings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:39:26.779165Z",
     "start_time": "2019-08-12T11:39:26.772689Z"
    },
    "code_folding": [
     0,
     2,
     10,
     15
    ]
   },
   "outputs": [],
   "source": [
    "def ratings_matrix(dictionary, to_df, transpose, fill_val):\n",
    "    \"\"\"\n",
    "    Inputs: dictionary: dictionary object, which should be in the form of:\n",
    "            key = contract ID, \n",
    "            value = clauses\n",
    "            \n",
    "            to_df: a boolean value, decides to output to pd.DataFrame object or numpy array\n",
    "            tranpose: a boolean value, decides to transpose the matrix or not (for sklearn)\n",
    "            fill_val: an int or np.nan object, decides what to fill matrix entries with\n",
    "\n",
    "    Output: ratings_matrix: pd.DataFrame object, which is in the form of:\n",
    "            rows = contracts, \n",
    "            columns = unique clauses,\n",
    "            values = 1 if there is a clause in a contract, else 0\n",
    "    \n",
    "    Notes: ratings_matrix() transforms a dictionary to a pd.DataFrame object with the form:\n",
    "           rows = contract ID\n",
    "           columns = clause #\n",
    "           cell value = clause (i.e the string)\n",
    "           \n",
    "           Then transforms into the ratings_matrix  \n",
    "    \"\"\"\n",
    "    \n",
    "    # Need to unpivot the df down to a very transactional form \n",
    "    print(\"Unpivoting...\")\n",
    "    df_to_melt = pd.DataFrame.from_dict(dictionary, orient = 'columns').T\n",
    "    melt_df = pd.melt(df_to_melt)\n",
    "    \n",
    "    # Create a column in melt_df to use as a counter to aggregate on\n",
    "    melt_df['count'] = 1\n",
    "    gc.collect()\n",
    "    print(\"Creating Ratings Matrix\")\n",
    "    ratings_matrix = pd.pivot_table(data = melt_df, \n",
    "                                    values = 'count', \n",
    "                                    columns = 'value',\n",
    "                                    index = 'variable',\n",
    "                                    fill_value = fill_val,\n",
    "                                   )\n",
    "    print(\"Matrix created of size:\", ratings_matrix.shape)\n",
    "    \n",
    "    if to_df == False:\n",
    "        print(\"Matrix is a numpy object\")\n",
    "        ratings_matrix = ratings_matrix.to_numpy()\n",
    "    else:\n",
    "        print(\"Matrix is a pandas object\")\n",
    "        #return ratings_matrix\n",
    "    \n",
    "    if transpose == True:\n",
    "        ratings_matrix = ratings_matrix.T\n",
    "        print(\"Matrix is transposed of size:\", ratings_matrix.shape)\n",
    "    \n",
    "    else:\n",
    "        print(\"Matrix not transposed\")\n",
    "    \n",
    "    return ratings_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:43:49.564927Z",
     "start_time": "2019-08-12T11:43:49.029646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sampled summarised data\n",
    "sampled_data = open(\"D:\\Python\\Thesis\\cf_ready_data\\samples\\\\summ_sample_5.0.pickle\",'rb')\n",
    "sampled_x = pickle.load(sampled_data)\n",
    "\n",
    "# Load sampled full data\n",
    "sampled_f_data = open(\"D:\\Python\\Thesis\\cf_ready_data\\samples\\\\full_sample_5.0.pickle\",'rb')\n",
    "sampled_f_x = pickle.load(sampled_f_data)\n",
    "\n",
    "# Memory saver\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:44:23.313269Z",
     "start_time": "2019-08-12T11:43:49.565919Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpivoting...\n",
      "Creating Ratings Matrix\n",
      "Matrix created of size: (2446, 23454)\n",
      "Matrix is a pandas object\n",
      "Matrix not transposed\n",
      "Unpivoting...\n",
      "Creating Ratings Matrix\n",
      "Matrix created of size: (2446, 23454)\n",
      "Matrix is a numpy object\n",
      "Matrix not transposed\n",
      "Unpivoting...\n",
      "Creating Ratings Matrix\n",
      "Matrix created of size: (2446, 25351)\n",
      "Matrix is a pandas object\n",
      "Matrix not transposed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix_df = ratings_matrix(sampled_x, to_df = True, transpose = False, fill_val = 0)\n",
    "rating_matrix = ratings_matrix(sampled_x, to_df = False, transpose = False, fill_val = 0)\n",
    "rating_matrix_orig = ratings_matrix(sampled_f_x, to_df = True, transpose = False, fill_val = 0)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export ratings matrix data to apply CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T11:44:37.045345Z",
     "start_time": "2019-08-12T11:44:23.315253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Export DF version\n",
    "rating_matrix_df.to_pickle(\"D:\\Python\\Thesis\\cf_ready_data\\\\\" + \"X\" + \"5_df.pickle\")\n",
    "\n",
    "# Export Matrix version\n",
    "rating_matrix_loc = open(\"D:\\Python\\Thesis\\cf_ready_data\\\\\" + \"X\" + \"5.pickle\",\"wb\")\n",
    "pickle.dump(rating_matrix, rating_matrix_loc)\n",
    "\n",
    "# Export orig DF version\n",
    "rating_matrix_orig.to_pickle(\"D:\\Python\\Thesis\\cf_ready_data\\\\\" + \"X\" + \"5_orig.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
